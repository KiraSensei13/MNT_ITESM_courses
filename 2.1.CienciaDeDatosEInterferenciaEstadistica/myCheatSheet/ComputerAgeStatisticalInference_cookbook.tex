%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% writeLaTeX Example: A quick guide to LaTeX
%
% Source: Dave Richeson (divisbyzero.com), Dickinson College
% 
% A one-size-fits-all LaTeX cheat sheet. Kept to two pages, so it 
% can be printed (double-sided) on one piece of paper
% 
% Feel free to distribute this example, but please keep the referral
% to divisbyzero.com
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{slashbox}


\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% -----------------------------------------------------------------------

\title{Quick Guide to LaTeX}

\begin{document}

\raggedright
\footnotesize

\begin{center}
     \Large{\textbf{Algorithms, Evidence, and Data Science Cookbook}} \\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

% -----------------------------------------------------------------------
\section{Part I: Classic Statistical Inference} % -----------------------
% -----------------------------------------------------------------------

* \textbf{Population:} the entire group

* \textbf{Sample:} a subset of the population

* \textbf{Mean:} 
$\mu$ is the mean of the population; $\bar{x}$ is the mean of the sample
$$ \frac{1}{n} \underset{i=1}{\overset{n}{\sum}}x_i $$

* \textbf{Variance:} the dispersion around the mean
\begin{multicols}{2}
Variance of a population:
$$ \sigma^2 = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \mu \right)}^2 $$

\pagebreak 
Variance of a sample:
$$ s^2 = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \bar{x} \right)}^2 $$
\end{multicols}

* \textbf{Standard Deviation:} square root of the variance

* \textbf{Standard Error:} an estimate of the standard deviation of the sampling distribution
\begin{multicols}{2}
For a mean:
$$ se(\bar{x}) = \sqrt{\frac{s^2}{n}} $$

\pagebreak 
For the difference between two means:
$$ se(\bar{x_1}, \bar{x_2}) = \sqrt{\frac{{s_1}^2}{n_1} + \frac{{s_2}^2}{n_2}} $$
\end{multicols}

% -----------------------------------------------------------------------
\subsection{Algorithms and Inference} % -------------------------------

* \textbf{Algorithm:} set of data probability-steps to produce an estimator \\
* \textbf{Inference:} measuring the uncertainty around the estimator \\

\emph{e.g.:} $\bar{x}$ the algorithm, while $se(\bar{x})$ is the inference 

% -----------------------------------------------------------------------
\subsubsection{A Regression Example}

\subsubsection{Linear Regression}
any regression is a conditional mean $\hat{Y_i} = E(Y_i|X_i)$ \\
* $Y:$ response variable \\
* $X:$ covariate/predictor/feature \\
* $\hat{\beta_0}, \hat{\beta_1}:$ regression coefficients \\
\begin{multicols}{2}
$$ \hat{\beta_0} = \hat{Y} - \hat{\beta_1} \hat{X} $$
$$ se(\hat{\beta_0}) = \hat{\sigma}^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2}\right] $$

\pagebreak 
$$ \hat{\beta_1} = \frac{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})(Y_i - \bar{Y})}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2} $$
$$ se(\hat{\beta_1}) = \frac{{\hat{\sigma}}^2}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2} $$
\end{multicols}
* predicted values = fitted curve given $x$:
$$\hat{Y}(x) = \hat{\beta_0} + \hat{\beta_1} x$$
* residuals $\hat{\epsilon}$:
$$ \hat{\epsilon}_i = Y_i - \hat{Y_i} = Y_i - \hat{\beta_0} + \beta_1 X_i $$
* residual sum of squares $RSS$
$$ RSS(\hat{\beta_0}, \hat{\beta_1}) = \underset{i=1}{\overset{n}{\sum }}\hat{\epsilon_i}^2 $$
* mean square error $\hat{\sigma}^2$
$$ \hat{\sigma}^2 = \frac{RSS(\hat{\beta_0}, \hat{\beta_1})}{n - 2} $$

\subsubsection{LOWESS \& LOESS}
* 1) specify the number of points within the range/window $n$
* 2) neighbour weightings $w(x_k)$
\begin{multicols}{2}
$$ w(x_k) = {\left(1 - {\left|\frac{x_i - x_k}{d}\right|}^{3}\right)}^{3} $$

\pagebreak 
$for k = 1, ..., n$ \\
$d$ is the distance between $x_i$ and the $k^{th}$ neighbouring point
\end{multicols}
* 3) for each range, estimate a regression function \\
LOWESS: $\hat{y_k} = a + b x_k$ \\
LOESS: $\hat{y_k} = a + b x_k + c {x_k}^2 $ \\
* 4) robust weightings $G(x_k)$
$$
G(x_k) = \begin{cases} \left(1 - \left(\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right)^2\right)^2, & \left|\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right| < 1 \\ 0, & \left|\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right| \geq 1 \end{cases}
$$

LOWESS: $\hat{y_k} = \underset{k}{\overset{}{\sum}} w(x_k) G(x_k) (a + b x_k)^2$ \\
LOESS: $\hat{y_k} = \underset{k}{\overset{}{\sum}} w(x_k) G(x_k) (a + b x_k + c {x_k}^2)^2$ \\
* 5) A series of new smoothed values is the result. The procedure can be repeated to get a more precise curve fitting.

\subsubsection{Bootstrapping}
* bootstrap principle: $\sigma_{(\text{sampling w/replacemnt})} = \sigma_{(\text{across samples})}$ \\
* bootstrap iterations: $B$ \\
* original sample: $(x_i, y_i)_{i = 1}^{N}$ \\
* bootstrap samples: $(x_{j(b)}, y_{j(b)})_{j \epsilon I}$ for $b = 1, ..., B$, $I = \{1, ..., N\}$, and $j$ is the index that is randomly sampled from I \\
* for each b, compute $\hat{y_{j(b)}}$ using LOWESS or any other model

\begin{center}
\begin{tabular}{ |cccccc| }
\hline
\backslashbox{j}{b} & 1 & 2 & 3 & $\cdots$ & $B$ \\

1 & $\bar{\hat{y_{1(1)}}}$ & $\bar{\hat{y_{1(2)}}}$ & $\bar{\hat{y_{1(3)}}}$ & $\cdots$ & $\bar{\hat{y_{1(B)}}}$ \\

2 & $\bar{\hat{y_{2(1)}}}$ & $\bar{\hat{y_{2(2)}}}$ & $\bar{\hat{y_{2(3)}}}$ & $\cdots$ & $\bar{\hat{y_{2(B)}}}$ \\

$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\

$N$ & $\bar{\hat{y_{N(1)}}}$ & $\bar{\hat{y_{N(2)}}}$ & $\bar{\hat{y_{N(3)}}}$ & $\cdots$ & $\bar{\hat{y_{N(B)}}}$ \\
\hline
\end{tabular}
\end{center}

* for each $j$ row, the standard deviation $\sigma_{j}^{boot}$ is \\
$$ \sigma_{j}^{boot} = \sqrt{\frac{(\bar{\hat{y_j}} - \bar{\bar{\hat{y_j}}})^2}{B - 1}} $$
* sort $\bar{\hat{i(b)}}$ by value from min to max $\rightarrow$ get the $5^{th}$ and $95^{th}$ values to get a $90\%$ confidence interval

% -----------------------------------------------------------------------
\subsubsection{Hypothesis Testing}

\subsection{T-test, one-sample}
* null hypothesis $H_o : \mu = \mu_0$ \\
* alternative hypothesis $H_a : \mu \{=, > or <\} \mu_0$ \\
* $t-statistic t$ standarices the difference between $\bar{x}$ and $\mu_0$
$$ t = \frac{\bar{x} - \mu_0}{se(\bar{x})} $$
degrees of freedom $df = n - 1$ \\
* $p-value$: probability that $\bar{x}$ was obtained by chance given $\mu_0 = \mu$. \\
* \textbf{algorithm:} read the t-distribution critical values (chart) for the $p-value$ using $t$ and $df$ \\
if($p-value < \alpha$)\{ reject $H_o$ and accept $H_a$ \} \\
else \{ cant reject $H_o$ \} \\
* $\alpha$ is the predetermined value of significance (usually 0.05) \\
* if($t$ is of the 'wrong' sign){$p-value = 1 - {p-value}_{chart}$}

\subsection{paired two-sample t-test}
each value of one group corresponds to a value in the other group\\
* \textbf{algorithm:} subtract the values for each sample to get one set of values and use $\mu_0$ to perform a one-sample t-test\\

\subsection{unpaired two-sample t-test}
the two populations are independent \\
* $H_o : \mu_1 = \mu_2$ \\
* $H_a : \mu_1 \{=, > or <\} \mu_2$ \\
* $t-statistic t$
$$ t = \frac{\bar{x_1} - \bar{x_2}}{se(\bar{x_1}, \bar{x_2})} $$
degrees of freedom $df = (n_1 - 1) + (n_2 - 1)$ \\
* \textbf{algorithm:} same as in one-sample t-test \\
* double the $p-value$ for $H_a : \mu_1 \neq \mu_2$ \\

\medskip 
* \textbf{Type I error $\alpha$:} probability of rejecting a true $H_o$ \\
* \textbf{Type II error $\beta$:} probability of failing to reject a false $H_o$ \\

% -----------------------------------------------------------------------
\subsubsection{Notes}

* the OLS confidence intervals work asymptotically $\rightarrow$ they assume the number of available observations is infinite, but it assumes normality \\
* in LOWESS, $n$ is not infinite, but it does not assume any distribution

% -----------------------------------------------------------------------
\subsection{Frequentist Inference} % ------------------------------------

* assumes the observed data comes from a probability distribution $F$\\
* $x = (x_1, ..., x_n)$ is the data vector (aka. \emph{the sample's values}) \\
* $X = (X_1, ..., X_n)$ is the vector of random variables (aka. \emph{a sample, individual draws of $F$}) \\
* the expectation property $\theta = E_F(X_i)$ (aka. the true expectation value of any draw $X_i$) \\
* $\hat{\theta}$ is the best estimate of $\theta$
\begin{multicols}{2}
$$ \hat{\theta} = t(x) $$

\pagebreak 
usually,
$$ t(x) = \bar{x} $$
where $t(x)$ is the algorithm
\end{multicols}
* $\hat{\theta}$ is sample specific, is a realization of $\hat{\Theta} = t(x)$. Typically,
\begin{multicols}{2}
$$ E_F(\hat{\Theta}) = \mu $$

\pagebreak 
$\mu$ is the expected value of producing an estimate using $t(x)$ when $x$ comes from $F$
\end{multicols}
* \textbf{Bias-Variance Trade-Off:} models with lower bias will have higher variance and vice versa. \\
* \textbf{Bias:} error from incorrect assumptions to make target function easier to learn (high bias $\rightarrow$ missing relevant relations or under-fitting)\\
* \textbf{Variance:} error from sensitivity to fluctuations in
the dataset, or how much the target estimate would
differ if different training data was used (high variance $\rightarrow$ modelling noise or over-fitting)
\begin{multicols}{2}
$$ bias = \mu - \theta$$
(aka. $expected - true values$)

\pagebreak 
$$ var = E_F\{(\hat{\Theta} - \mu)^2\} $$
\end{multicols}

\subsubsection{Frequentist principles}
* usually defines parameters with infinite sequence of trials $\rightarrow$ hypothetical data sets $X^{(1)}, X^{(2)}, \hdots$ generate infinite samples ${\hat{\Theta}}^{(1)}, {\hat{\Theta}}^{(2)}, \hdots$
* 1) Plug-in principle: relate the sample $se(\bar{x})$ with the true variance.
$$ {var}_F(x) = \hat{{var}_F} = \frac{1}{n - 1} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \bar{x} \right)^2} $$
$$ se(\bar{x}) = \left[\frac{{var}_F(x)}{n}\right]^{\frac{1}{2}} $$
* 2) Taylor series approximations: relate $t(x)$ by local linear approximations (aka. compute $\bar{se}(x)$ of the transformed estimator)
$$ se(\hat{\theta}) = se(\bar{x}) \frac{d \hat{\theta}}{d \bar{x}} = se(\bar{x}) \frac{d t(x)}{d \bar{x}} $$
* 3.1) Parametric Families: given $x = (x_1, ..., x_n)$, the Likelihood Function $L(x)$ (aka. the probability to observe x) is given by: \\
\emph{e.g.} $\hat{\theta} = \mu$ for a normal distribution
$$ P(x | N(\mu, \sigma^2)) = P(x_1 | N(\mu, \sigma^2)) ... P(x_n | N(\mu, \sigma^2)) $$
$$ P(x | N(\mu, \sigma^2)) = \left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)^n  \prod _{1=1}^n e^{-\frac{(x_i - \mu)^2}{2 \sigma^2}} = L(x)$$
* 3.2) MLE (maximum likelihood estimate): find $\hat{\theta}$ such that $L(x)$ is maximized \\
\emph{e.g.}
$$ \overset{\max}{\hat{\theta}} L(x) \Rightarrow \overset{\max}{\mu} L(x) = \hat{\mu}^{MLE}$$
* 4) Simulation and Bootstrap: estimate $F$ as $\hat{F}$, then simulate values from $\hat{F}$ to get a prior sample $\hat{\Theta}^{(k)} = t(x^{(b)})$ \\
The empirical standard deviation of the $\hat{\Theta}'s$ is the frequentist estimate for $se(\hat{\theta})$ \\
* 5) Pivotal Statistics: Frequentist use pivotal statistics whenever they are available to conduct stat. tests \\
\emph{e.g.} t-test is a pivotal statistic as it does not depend on parameters the distribution might have.

% -----------------------------------------------------------------------
\subsubsection{Frequentist Optimality}

Neyman-Pearson lemma optimum hypothesis-testing algorithm: \\
purpose: choose one of the two possible density functions for observed data $x$ \\
* null hypothesis density $f_0(x)$ \\
* alternative density $f_1(x)$ \\
let $L(x)$ be the Likelihood Ratio
$$ L(X) = \frac{f_1(X)}{f_0(X)} $$
let the testing rule $t_c{x}$ be:
$$
t_c{x} = \begin{cases} 1(pic f_1(x)), & ln(L(X)) \geq c \\ 0(pic f_0(x)), & ln(L(X)) < c \end{cases}
$$
* only rules in the $t_c{x}$ form can be optimal
\emph{prblem Steps} \\
* 1) define the density functions $f_0(x_i)$ and $f_1(x_i)$ for $f_0(x)$ and $f_1(x)$ \\
\emph{e.g.}
\begin{multicols}{2}
$$ f_0 \sim N(\mu_0,{\sigma^2}_0) $$
$$ f_0 \sim N(0,1) $$
$$ f_0(x_i) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{{x_i}^2}{2}} $$

\pagebreak 
$$ f_1 \sim N(\mu_1,{\sigma^2}_1) $$
$$ f_1 \sim N(0.5,1) $$
$$ f_1(x_i) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{{x_i - 0.5}^2}{2}} $$
\end{multicols}
* 2) calculate the likelihood functions $f_0(X)$ and $f_1(X)$ \\
\emph{e.g.}
$$ f_0(X)) = \left[\frac{1}{\sqrt{2 \pi}}\right]^n e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum}} {x_i}^2} $$
$$ f_1(X) = \left[\frac{1}{\sqrt{2 \pi}}\right]^n e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum}} ((x_i - 0.5)^2)} $$
* 3) calculate the likelihood ratio \\
\emph{e.g.}
$$ L(X) = \frac{e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum} ((x_i - 0.5)^2})}}{e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum} {x_i}^2}}} $$
$$ L(X) = e^{-\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right]} $$
* 4) remove all independent variables
\emph{e.g.} \\
\begin{multicols}{2}
$$ L(X) > c \Rightarrow $$
only the mean depends on the sample $x$

\pagebreak 
$$ e^{-\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right]} > c_1 $$
$$ -\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right] > C_2 $$
$$ n \bar{x} - \frac{n}{4} > c_3 $$
$$ \bar{x} > c_4 $$
$$ \bar{x} > c $$
\end{multicols}
* 5) the \emph{most powerful} hypothesis test at any type I error rate $\alpha$ is to compare $c$ to a constant. \\
\emph{e.g.} \\
$$ \alpha = P(\bar{x} > c | \mu = \mu_0) $$
$$ \alpha = P((\bar{x} - \mu)\sqrt{n} > (c - \mu)\sqrt{n} | \mu = 0) $$
$$ \alpha = 1 - P(\bar{x}\sqrt{n} \leq c\sqrt{n} | \mu = 0) $$
$$ \alpha = 1 - \Phi(c\sqrt{n}) $$
$\Phi$ is the cumulative density function (CDF) of a normal distribution $N(\mu_0,{\sigma^2}_0)$ \\

* 6) calculate $c$
\begin{multicols}{2}
\emph{e.g.}
$$ \Phi(c \sqrt{n}) = 1 - \alpha $$
$$ c \sqrt{n} = \Phi^{-1}(1 - \alpha) $$
$$ c = 0 + \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha) $$

\pagebreak 
In general:\\
$$ c = \mu_0 + \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha) $$
\end{multicols}
* 7) calculate $\beta$, such that it's minimized \\
\emph{e.g.} \\
$$ \beta = P(\bar{x} \leq c | \mu = \mu_1) $$
$$ \beta = P((\bar{x} - \mu) \sqrt{n} \leq (c - \mu) \sqrt{n} | \mu = 0.5) $$
$$ \beta = \Phi ((c - 0.5) \sqrt{n} ) $$

% -----------------------------------------------------------------------
\subsubsection{Notes and Details} 
* $1 - \beta$ is the power of the hypothesis test (probability of correctly rejecting $f_0(x)$)

% -----------------------------------------------------------------------
\subsection{Bayesian Inference} % ---------------------------------------

\subsubsection{Bayes Rule}
$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

* Bayes Rule (for one $\mu$) can be written as:
\begin{multicols}{2}
$$ g(\mu | x) = c_x L_x(\mu) g(\mu) $$

\pagebreak 
where: \\
$\mu:$ an unobserved point in the parameter space $\Omega$ \\
$x:$ a point in the sample space $X$ \\
$c_x:$ normalization constant of the posterior distribution \\
$g(\mu | x):$ posterior distribution
$L_x(\mu):$ likelihood function
$g(\mu):$ prior distribution
\end{multicols}
* Bayes Rule (for two $\mu_1$, $\mu_2$) can be written as:
\begin{multicols}{2}
$$ \frac{g(\mu_1 | x)}{g(\mu_2 | x)} = \frac{g(\mu_1)}{g(\mu_2)} \frac{L_x(\mu_1)}{L_x(\mu_2)} $$

\pagebreak 
The posterior odds ratio is the prior odds ratio times the likelihood ratio
\end{multicols}

$$ L_x(\mu) = \prod _{1=1}^n e^{-\frac{1}{2}(x_i - \mu)^2} $$

% -----------------------------------------------------------------------
\subsubsection{Warm-up example}
\emph{e.g.} Find the probability of identical twins. The doctor says that $\frac{1}{3}$ of twin births are identical. A sonogram observed same sex. identical twins are of the same sex, while fraternals have 0.5 probability to be of the same sex.

$$ \frac{g(identical | sameSex)}{g(fraternal | sameSex)} = \frac{g(identical)}{g(fraternal)} \times \frac{L_{identical}(sameSex)}{L_{fraternal}(sameSex)} $$
$$ \frac{g(identical | sameSex)}{g(fraternal | sameSex)} = \frac{\frac{1}{3}}{1 - \frac{1}{3}} \times \frac{1}{\frac{1}{2}} $$

% -----------------------------------------------------------------------
\subsubsection{Flaws in Frequentist Inference}

* In Frequentist, if the algorith changes (even if the data points stay exactly the same), the significance level is different for each algorithm.

* On Bayesian inference, the algorithm stays the same $\rightarrow$ the significance level does not change.

% -----------------------------------------------------------------------
\subsubsection{A Bayesian/Frequentist Comparison List}
\begin{multicols}{2}
\textbf{Bayesian:}

* attention is in choosing an algorithm $t(x)$

* operates only in one sample with the whole parameter space

* answers all posible questions at once, since the posterior is a distribution

\pagebreak 
\textbf{Frequentist:}

* attention is in choosing a prior $\prod$

* operates with one parameter (specific question) in many samples

* only computes the expected value and the variance (each answer requires an specific algorithm)

* is more flexible than Bayes as we can come up with many algorithms
\end{multicols}

% -----------------------------------------------------------------------
\subsubsection{Notes and Details}

* like in frequentist, the fundamental unit of inference is a family of probability densities.

* Bayesian inferences assumes the knowledge of a prior density $g(\mu), \mu \epsilon \Omega$

% -----------------------------------------------------------------------
\subsection{Fisherian Inference and Maximum Likelihood Estimation} % ----



% -----------------------------------------------------------------------
\subsubsection{Likelihood and Maximum Likelihood}

% -----------------------------------------------------------------------
\subsubsection{Fisher Information and the MLE}

% -----------------------------------------------------------------------
\subsubsection{Conditional Inference}

% -----------------------------------------------------------------------
\subsubsection{Permutation and Randomization}

% -----------------------------------------------------------------------
\subsubsection{Notes and Details}

% -----------------------------------------------------------------------
\subsection{Parametric Models and Exponential Families} % ---------------

% -----------------------------------------------------------------------
\subsubsection{Univariate Families}

% -----------------------------------------------------------------------
\section{} % ------------------------------------------------------------
\hrule
Osamu Katagiri - A01212611, \href{https://www.linkedin.com/in/osamu-katagiri-84b2b940/}{linkedin.com/osamu-katagiri/}
\end{multicols}

\end{document}
