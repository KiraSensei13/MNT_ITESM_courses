%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% writeLaTeX Example: A quick guide to LaTeX
%
% Source: Dave Richeson (divisbyzero.com), Dickinson College
% 
% A one-size-fits-all LaTeX cheat sheet. Kept to two pages, so it 
% can be printed (double-sided) on one piece of paper
% 
% Feel free to distribute this example, but please keep the referral
% to divisbyzero.com
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{slashbox}


\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% -----------------------------------------------------------------------

\title{Quick Guide to LaTeX}

\begin{document}

\raggedright
\footnotesize

\begin{center}
     \Large{\textbf{Algorithms, Evidence, and Data Science Cookbook}} \\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

% -----------------------------------------------------------------------
\section{Part I: Classic Statistical Inference} % -----------------------
% -----------------------------------------------------------------------

* \textbf{Population:} the entire group

* \textbf{Sample:} a subset of the population

* \textbf{Mean:} 
$\mu$ is the mean of the population; $\bar{x}$ is the mean of the sample
$$ \frac{1}{n} \underset{i=1}{\overset{n}{\sum}}x_i $$

* \textbf{Variance:} the dispersion around the mean
\begin{multicols}{2}
Variance of a population:
$$ \sigma^2 = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \mu \right)}^2 $$

\pagebreak 
Variance of a sample:
$$ s^2 = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \bar{x} \right)}^2 $$
\end{multicols}

* \textbf{Standard Deviation:} square root of the variance

* \textbf{Standard Error:} an estimate of the standard deviation of the sampling distribution
\begin{multicols}{2}
For a mean:
$$ se(\bar{x}) = \sqrt{\frac{s^2}{n}} $$

\pagebreak 
For the difference between two means:
$$ se(\bar{x_1}, \bar{x_2}) = \sqrt{\frac{{s_1}^2}{n_1} + \frac{{s_2}^2}{n_2}} $$
\end{multicols}

% -----------------------------------------------------------------------
\subsection{Algorithms and Inference} % -------------------------------

* \textbf{Algorithm:} set of data probability-steps to produce an estimator \\
* \textbf{Inference:} measuring the uncertainty around the estimator \\

\emph{e.g.:} $\bar{x}$ the algorithm, while $se(\bar{x})$ is the inference 

% -----------------------------------------------------------------------
\subsubsection{A Regression Example}

\subsubsection{Linear Regression}
any regression is a conditional mean $\hat{Y_i} = E(Y_i|X_i)$ \\
* $Y:$ response variable \\
* $X:$ covariate/predictor/feature \\
* $\hat{\beta_0}, \hat{\beta_1}:$ regression coefficients \\
\begin{multicols}{2}
$$ \hat{\beta_0} = \hat{Y} - \hat{\beta_1} \hat{X} $$
$$ se(\hat{\beta_0}) = \hat{\sigma}^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2}\right] $$

\pagebreak 
$$ \hat{\beta_1} = \frac{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})(Y_i - \bar{Y})}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2} $$
$$ se(\hat{\beta_1}) = \frac{{\hat{\sigma}}^2}{\underset{i=1}{\overset{n}{\sum }}(X_i - \bar{X})^2} $$
\end{multicols}
* predicted values = fitted curve given $x$:
$$\hat{Y}(x) = \hat{\beta_0} + \hat{\beta_1} x$$
* residuals $\hat{\epsilon}$:
$$ \hat{\epsilon}_i = Y_i - \hat{Y_i} = Y_i - \hat{\beta_0} + \beta_1 X_i $$
* residual sum of squares $RSS$
$$ RSS(\hat{\beta_0}, \hat{\beta_1}) = \underset{i=1}{\overset{n}{\sum }}\hat{\epsilon_i}^2 $$
* mean square error $\hat{\sigma}^2$
$$ \hat{\sigma}^2 = \frac{RSS(\hat{\beta_0}, \hat{\beta_1})}{n - 2} $$

\subsubsection{LOWESS \& LOESS}
* 1) specify the number of points within the range/window $n$
* 2) neighbour weightings $w(x_k)$
\begin{multicols}{2}
$$ w(x_k) = {\left(1 - {\left|\frac{x_i - x_k}{d}\right|}^{3}\right)}^{3} $$

\pagebreak 
$for k = 1, ..., n$ \\
$d$ is the distance between $x_i$ and the $k^{th}$ neighbouring point
\end{multicols}
* 3) for each range, estimate a regression function \\
LOWESS: $\hat{y_k} = a + b x_k$ \\
LOESS: $\hat{y_k} = a + b x_k + c {x_k}^2 $ \\
* 4) robust weightings $G(x_k)$
$$
G(x_k) = \begin{cases} \left(1 - \left(\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right)^2\right)^2, & \left|\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right| < 1 \\ 0, & \left|\frac{\left|y_i - \hat{y_i}\right|}{6 median(\left|y_i - \hat{y_i}\right|)}\right| \geq 1 \end{cases}
$$

LOWESS: $\hat{y_k} = \underset{k}{\overset{}{\sum}} w(x_k) G(x_k) (a + b x_k)^2$ \\
LOESS: $\hat{y_k} = \underset{k}{\overset{}{\sum}} w(x_k) G(x_k) (a + b x_k + c {x_k}^2)^2$ \\
* 5) A series of new smoothed values is the result. The procedure can be repeated to get a more precise curve fitting.

\subsubsection{Bootstrapping}
* bootstrap principle: $\sigma_{(\text{sampling w/replacemnt})} = \sigma_{(\text{across samples})}$ \\
* bootstrap iterations: $B$ \\
* original sample: $(x_i, y_i)_{i = 1}^{N}$ \\
* bootstrap samples: $(x_{j(b)}, y_{j(b)})_{j \epsilon I}$ for $b = 1, ..., B$, $I = \{1, ..., N\}$, and $j$ is the index that is randomly sampled from I \\
* for each b, compute $\hat{y_{j(b)}}$ using LOWESS or any other model

\begin{center}
\begin{tabular}{ |cccccc| }
\hline
\backslashbox{j}{b} & 1 & 2 & 3 & $\cdots$ & $B$ \\

1 & $\bar{\hat{y_{1(1)}}}$ & $\bar{\hat{y_{1(2)}}}$ & $\bar{\hat{y_{1(3)}}}$ & $\cdots$ & $\bar{\hat{y_{1(B)}}}$ \\

2 & $\bar{\hat{y_{2(1)}}}$ & $\bar{\hat{y_{2(2)}}}$ & $\bar{\hat{y_{2(3)}}}$ & $\cdots$ & $\bar{\hat{y_{2(B)}}}$ \\

$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\

$N$ & $\bar{\hat{y_{N(1)}}}$ & $\bar{\hat{y_{N(2)}}}$ & $\bar{\hat{y_{N(3)}}}$ & $\cdots$ & $\bar{\hat{y_{N(B)}}}$ \\
\hline
\end{tabular}
\end{center}

* for each $j$ row, the standard deviation $\sigma_{j}^{boot}$ is \\
$$ \sigma_{j}^{boot} = \sqrt{\frac{(\bar{\hat{y_j}} - \bar{\bar{\hat{y_j}}})^2}{B - 1}} $$
* sort $\bar{\hat{i(b)}}$ by value from min to max $\rightarrow$ get the $5^{th}$ and $95^{th}$ values to get a $90\%$ confidence interval

% -----------------------------------------------------------------------
\subsubsection{Hypothesis Testing}

\subsection{T-test, one-sample}
* null hypothesis $H_o : \mu = \mu_0$ \\
* alternative hypothesis $H_a : \mu \{=, > or <\} \mu_0$ \\
* $t-statistic t$ standarices the difference between $\bar{x}$ and $\mu_0$
$$ t = \frac{\bar{x} - \mu_0}{se(\bar{x})} $$
degrees of freedom $df = n - 1$ \\
* $p-value$: probability that $\bar{x}$ was obtained by chance given $\mu_0 = \mu$. \\
* \textbf{algorithm:} read the t-distribution critical values (chart) for the $p-value$ using $t$ and $df$ \\
if($p-value < \alpha$)\{ reject $H_o$ and accept $H_a$ \} \\
else \{ cant reject $H_o$ \} \\
* $\alpha$ is the predetermined value of significance (usually 0.05) \\
* if($t$ is of the 'wrong' sign){$p-value = 1 - {p-value}_{chart}$}

\subsection{paired two-sample t-test}
each value of one group corresponds to a value in the other group\\
* \textbf{algorithm:} subtract the values for each sample to get one set of values and use $\mu_0$ to perform a one-sample t-test\\

\subsection{unpaired two-sample t-test}
the two populations are independent \\
* $H_o : \mu_1 = \mu_2$ \\
* $H_a : \mu_1 \{=, > or <\} \mu_2$ \\
* $t-statistic t$
$$ t = \frac{\bar{x_1} - \bar{x_2}}{se(\bar{x_1}, \bar{x_2})} $$
degrees of freedom $df = (n_1 - 1) + (n_2 - 1)$ \\
* \textbf{algorithm:} same as in one-sample t-test \\
* double the $p-value$ for $H_a : \mu_1 \neq \mu_2$ \\

\medskip 
* \textbf{Type I error $\alpha$:} probability of rejecting a true $H_o$ \\
* \textbf{Type II error $\beta$:} probability of failing to reject a false $H_o$ \\

% -----------------------------------------------------------------------
\subsubsection{Notes}

* the OLS confidence intervals work asymptotically $\rightarrow$ they assume the number of available observations is infinite, but it assumes normality \\
* in LOWESS, $n$ is not infinite, but it does not assume any distribution

% -----------------------------------------------------------------------
\subsection{Frequentist Inference} % ------------------------------------

* assumes the observed data comes from a probability distribution $F$\\
* $x = (x_1, ..., x_n)$ is the data vector (aka. \emph{the sample's values}) \\
* $X = (X_1, ..., X_n)$ is the vector of random variables (aka. \emph{a sample, individual draws of $F$}) \\
* the expectation property $\theta = E_F(X_i)$ (aka. the true expectation value of any draw $X_i$) \\
* $\hat{\theta}$ is the best estimate of $\theta$
\begin{multicols}{2}
$$ \hat{\theta} = t(x) $$

\pagebreak 
usually,
$$ t(x) = \bar{x} $$
where $t(x)$ is the algorithm
\end{multicols}
* $\hat{\theta}$ is sample specific, is a realization of $\hat{\Theta} = t(x)$. Typically,
\begin{multicols}{2}
$$ E_F(\hat{\Theta}) = \mu $$

\pagebreak 
$\mu$ is the expected value of producing an estimate using $t(x)$ when $x$ comes from $F$
\end{multicols}
* \textbf{Bias-Variance Trade-Off:} models with lower bias will have higher variance and vice versa. \\
* \textbf{Bias:} error from incorrect assumptions to make target function easier to learn (high bias $\rightarrow$ missing relevant relations or under-fitting)\\
* \textbf{Variance:} error from sensitivity to fluctuations in
the dataset, or how much the target estimate would
differ if different training data was used (high variance $\rightarrow$ modelling noise or over-fitting)
\begin{multicols}{2}
$$ bias = \mu - \theta$$
(aka. $expected - true values$)

\pagebreak 
$$ var = E_F\{(\hat{\Theta} - \mu)^2\} $$
\end{multicols}

\subsubsection{Frequentist principles}
* usually defines parameters with infinite sequence of trials $\rightarrow$ hypothetical data sets $X^{(1)}, X^{(2)}, \hdots$ generate infinite samples ${\hat{\Theta}}^{(1)}, {\hat{\Theta}}^{(2)}, \hdots$
* 1) Plug-in principle: relate the sample $se(\bar{x})$ with the true variance.
$$ {var}_F(x) = \hat{{var}_F} = \frac{1}{n - 1} \underset{i=1}{\overset{n}{\sum}} {\left(x_i - \bar{x} \right)^2} $$
$$ se(\bar{x}) = \left[\frac{{var}_F(x)}{n}\right]^{\frac{1}{2}} $$
* 2) Taylor series approximations: relate $t(x)$ by local linear approximations (aka. compute $\bar{se}(x)$ of the transformed estimator)
$$ se(\hat{\theta}) = se(\bar{x}) \frac{d \hat{\theta}}{d \bar{x}} = se(\bar{x}) \frac{d t(x)}{d \bar{x}} $$
* 3.1) Parametric Families: given $x = (x_1, ..., x_n)$, the Likelihood Function $L(x)$ (aka. the probability to observe x) is given by: \\
\emph{e.g.} $\hat{\theta} = \mu$ for a normal distribution
$$ P(x | N(\mu, \sigma^2)) = P(x_1 | N(\mu, \sigma^2)) ... P(x_n | N(\mu, \sigma^2)) $$
$$ P(x | N(\mu, \sigma^2)) = \left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)^n  \prod _{1=1}^n e^{-\frac{(x_i - \mu)^2}{2 \sigma^2}} = L(x)$$
$$ L(x) = \prod _{1=1}^n f_{\theta}(x_i) $$
where $f_{\theta}$ is the density function \\
* 3.2) MLE (maximum likelihood estimate): find $\hat{\theta}$ such that $L(x)$ is maximized \\
\emph{e.g.}
$$ \overset{\max}{\hat{\theta}} L(x) \Rightarrow \overset{\max}{\mu} L(x) = \hat{\mu}^{MLE}$$
* 4) Simulation and Bootstrap: estimate $F$ as $\hat{F}$, then simulate values from $\hat{F}$ to get a prior sample $\hat{\Theta}^{(k)} = t(x^{(b)})$ \\
The empirical standard deviation of the $\hat{\Theta}'s$ is the frequentist estimate for $se(\hat{\theta})$ \\
* 5) Pivotal Statistics: Frequentist use pivotal statistics whenever they are available to conduct stat. tests \\
\emph{e.g.} t-test is a pivotal statistic as it does not depend on parameters the distribution might have.

% -----------------------------------------------------------------------
\subsubsection{Frequentist Optimality}

Neyman-Pearson lemma optimum hypothesis-testing algorithm: \\
purpose: choose one of the two possible density functions for observed data $x$ \\
* null hypothesis density $f_0(x)$ \\
* alternative density $f_1(x)$ \\
let $L(x)$ be the Likelihood Ratio
$$ L(X) = \frac{f_1(X)}{f_0(X)} $$
let the testing rule $t_c{x}$ be:
$$
t_c{x} = \begin{cases} 1(pic f_1(x)), & ln(L(X)) \geq c \\ 0(pic f_0(x)), & ln(L(X)) < c \end{cases}
$$
* only rules in the $t_c{x}$ form can be optimal
\emph{prblem Steps} \\
* 1) define the density functions $f_0(x_i)$ and $f_1(x_i)$ for $f_0(x)$ and $f_1(x)$ \\
\emph{e.g.}
\begin{multicols}{2}
$$ f_0 \sim N(\mu_0,{\sigma^2}_0) $$
$$ f_0 \sim N(0,1) $$
$$ f_0(x_i) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{{x_i}^2}{2}} $$

\pagebreak 
$$ f_1 \sim N(\mu_1,{\sigma^2}_1) $$
$$ f_1 \sim N(0.5,1) $$
$$ f_1(x_i) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{{x_i - 0.5}^2}{2}} $$
\end{multicols}
* 2) calculate the likelihood functions $f_0(X)$ and $f_1(X)$ \\
\emph{e.g.}
$$ f_0(X)) = \left[\frac{1}{\sqrt{2 \pi}}\right]^n e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum}} {x_i}^2} $$
$$ f_1(X) = \left[\frac{1}{\sqrt{2 \pi}}\right]^n e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum}} ((x_i - 0.5)^2)} $$
* 3) calculate the likelihood ratio \\
\emph{e.g.}
$$ L(X) = \frac{e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum} ((x_i - 0.5)^2})}}{e^{-\frac{1}{2} \underset{i=1}{\overset{n}{\sum} {x_i}^2}}} $$
$$ L(X) = e^{-\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right]} $$
* 4) remove all independent variables
\emph{e.g.} \\
\begin{multicols}{2}
$$ L(X) > c \Rightarrow $$
only the mean depends on the sample $x$

\pagebreak 
$$ e^{-\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right]} > c_1 $$
$$ -\frac{1}{2} \left[n \bar{x} - \frac{n}{4}\right] > C_2 $$
$$ n \bar{x} - \frac{n}{4} > c_3 $$
$$ \bar{x} > c_4 $$
$$ \bar{x} > c $$
\end{multicols}
* 5) the \emph{most powerful} hypothesis test at any type I error rate $\alpha$ is to compare $c$ to a constant. \\
\emph{e.g.} \\
$$ \alpha = P(\bar{x} > c | \mu = \mu_0) $$
$$ \alpha = P((\bar{x} - \mu)\sqrt{n} > (c - \mu)\sqrt{n} | \mu = 0) $$
$$ \alpha = 1 - P(\bar{x}\sqrt{n} \leq c\sqrt{n} | \mu = 0) $$
$$ \alpha = 1 - \Phi(c\sqrt{n}) $$
$\Phi$ is the cumulative density function (CDF) of a normal distribution $N(\mu_0,{\sigma^2}_0)$ \\

* 6) calculate $c$
\begin{multicols}{2}
\emph{e.g.}
$$ \Phi(c \sqrt{n}) = 1 - \alpha $$
$$ c \sqrt{n} = \Phi^{-1}(1 - \alpha) $$
$$ c = 0 + \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha) $$

\pagebreak 
In general:\\
$$ c = \mu_0 + \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha) $$
\end{multicols}
* 7) calculate $\beta$, such that it's minimized \\
\emph{e.g.} \\
$$ \beta = P(\bar{x} \leq c | \mu = \mu_1) $$
$$ \beta = P((\bar{x} - \mu) \sqrt{n} \leq (c - \mu) \sqrt{n} | \mu = 0.5) $$
$$ \beta = \Phi ((c - 0.5) \sqrt{n} ) $$

% -----------------------------------------------------------------------
\subsubsection{Notes and Details} 
* $1 - \beta$ is the power of the hypothesis test (probability of correctly rejecting $f_0(x)$)

% -----------------------------------------------------------------------
\subsection{Bayesian Inference} % ---------------------------------------

\subsubsection{Bayes Rule}
$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

* Bayes Rule (for one $\mu$) can be written as:
\begin{multicols}{2}
$$ g(\mu | x) = c_x L_x(\mu) g(\mu) $$

\pagebreak 
where: \\
$\mu:$ an unobserved point in the parameter space $\Omega$ \\
$x:$ a point in the sample space $X$ \\
$c_x:$ normalization constant of the posterior distribution \\
$g(\mu | x):$ posterior distribution
$L_x(\mu):$ likelihood function
$g(\mu):$ prior distribution
\end{multicols}
* Bayes Rule (for two $\mu_1$, $\mu_2$) can be written as:
\begin{multicols}{2}
$$ \frac{g(\mu_1 | x)}{g(\mu_2 | x)} = \frac{g(\mu_1)}{g(\mu_2)} \frac{L_x(\mu_1)}{L_x(\mu_2)} $$

\pagebreak 
The posterior odds ratio is the prior odds ratio times the likelihood ratio
\end{multicols}

$$ L_x(\mu) = \prod _{i=1}^n e^{-\frac{1}{2}(x_i - \mu)^2} $$

% -----------------------------------------------------------------------
\subsubsection{Warm-up example}
\emph{e.g.} Find the probability of identical twins. The doctor says that $\frac{1}{3}$ of twin births are identical. A sonogram observed same sex. identical twins are of the same sex, while fraternals have 0.5 probability to be of the same sex.

$$ \frac{g(identical | sameSex)}{g(fraternal | sameSex)} = \frac{g(identical)}{g(fraternal)} \times \frac{L_{identical}(sameSex)}{L_{fraternal}(sameSex)} $$
$$ \frac{g(identical | sameSex)}{g(fraternal | sameSex)} = \frac{\frac{1}{3}}{1 - \frac{1}{3}} \times \frac{1}{\frac{1}{2}} $$

% -----------------------------------------------------------------------
\subsubsection{Flaws in Frequentist Inference}

* In Frequentist, if the algorith changes (even if the data points stay exactly the same), the significance level is different for each algorithm.

* On Bayesian inference, the algorithm stays the same $\rightarrow$ the significance level does not change.

% -----------------------------------------------------------------------
\subsubsection{A Bayesian/Frequentist Comparison List}
\begin{multicols}{2}
\textbf{Bayesian:}

* attention is in choosing an algorithm $t(x)$

* operates only in one sample with the whole parameter space

* answers all posible questions at once, since the posterior is a distribution

\pagebreak 
\textbf{Frequentist:}

* attention is in choosing a prior $\Pi$

* operates with one parameter (specific question) in many samples

* only computes the expected value and the variance (each answer requires an specific algorithm)

* is more flexible than Bayes as we can come up with many algorithms
\end{multicols}

% -----------------------------------------------------------------------
\subsubsection{Notes and Details}

* like in frequentist, the fundamental unit of inference is a family of probability densities.

* Bayesian inferences assumes the knowledge of a prior density $g(\mu), \mu \epsilon \Omega$

% -----------------------------------------------------------------------
\subsection{Fisherian Inference and Maximum Likelihood Estimation} % ----

* The log-likelihood function is defined as:
\begin{multicols}{2}
$$ \ell_x(\theta) = Log\{f_{\theta}(x)\} $$
for a fixed $x$ and a variable $\theta$

\pagebreak 
$\ell_x(\theta):$ gets the most likely parameters to get the sample $x$
$f_{\theta}(x):$ likelihood function (aka. family probability densities)
$\theta:$ vector of parameters
\end{multicols}

* The $MLE$ is the value of $\theta \epsilon \Omega$ that maximizes $\ell_x(\theta)$
$$ MLE:\hat{\theta} = \overset{argmax}{\theta \epsilon \Omega}\{\ell_x(\theta)\}$$

* Estimate functions of the true parameter: $\hat{\gamma} = T(\hat{\theta})$

* Good frequentist properties (good bias \& variance):
\begin{multicols}{2}
$$bias = \mu - E(\hat{\mu})$$
$\mu:$ true value of the parameter \\
$E(\hat{\mu}):$ expected value of the estimate

\pagebreak 
$$variance = \underset{i=1}{\overset{I}{\sum}} ({\hat{\mu}}^{(i)} - E(\hat{\mu}))^2$$
$$variance = E_F\{({\hat{\mu}}^{(i)} - E(\hat{\mu}))^2\}$$
\end{multicols}

* Reasonable Bayesian justification
\begin{multicols}{2}
$$ P(\theta | x) = c_x \Pi(\theta) e^{\ell_x(\theta)} $$

\pagebreak 
$P(\theta | x):$ posterior \\
$c_x:$ constant \\
$\Pi(\theta):$ prior \\
$e^{\ell_x(\theta)}:$ maximum likelihood estimation
\end{multicols}

* Fisherian inference assumes a flat prior (aka. unknown prior), so that the MLE $\hat{\theta}^{MLE}$ is a maximizer of $P(\theta | x)$. (The MLE is the highest point of the posterior distribution)

* As the algorithm does not change, the significance level is not affected by unexpected changes in the algorithm.

\medskip 
\emph{e.g.} - for a Normal density function\\
* let $\theta = (\mu, \sigma^2)$ \\
* density function $f_{\theta} = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2} \left(\frac{x_i - \mu}{\sigma}\right)^2}$
* Since: $L(x) = \prod _{1=1}^n f_{\theta}(x_i)$
Log-Likelihood function $\ell_x(\theta) = \underset{i=1}{\overset{n}{\sum}} Log\{f_{\theta}(x_i)\} = \underset{i=1}{\overset{I}{\sum}} \ell_x(\theta)$
$$ \hat{\mu^{MLE}} = \bar{x} $$
$$ \sigma^{MLE} = \sqrt{\frac{\underset{i=1}{\overset{n}{\sum}} (x_i - \bar{x})^2}{n}} $$

* MLE can cause over-fitting identification problems when we fit a lot of parameters in $\theta$ (it would become very specific to our sample $\rightarrow$ may not represent the population)
% -----------------------------------------------------------------------
\subsubsection{Fisher Information and the MLE}

Log-Likelihood Function
$$ \ell_x(\theta) = Log f_{\theta}(x) $$

Score Function \\
how higher or lower is the likelihood function value of the sample as $\theta$ varies?
$$ \dot{\ell}_x(\theta) = \frac{\dot{f_{\theta}}(x)}{f_{\theta}(x)} $$

Expectation of $\dot{\ell}_x(\theta)$
\begin{multicols}{2}
$$ E(x) = \int_x x f(x)\, dx $$

\pagebreak 
$f(x):$ density function
\end{multicols}
$$ E[\dot{\ell}_x(\theta)] = 0 $$

Variance of $\dot{\ell}_x(\theta)$
$$ V[x] = \int_x \left[x - E(x)\right]^2 f(x)\, dx $$
$$ V[\dot{\ell}_x(\theta)] = \int_x \left[\dot{\ell}_x(\theta)\right]^2 f_{\theta}(x) \, dx $$

Fisher Information $I_0$
$$ I_0 = V[\dot{\ell}_x(\theta)] $$

\begin{multicols}{2}
$$ \ddot{\ell}_x(\theta) = \frac{\ddot{f_{\theta}}(x)}{f_{\theta}(x)} - \left(\frac{\dot{f_{\theta}}(x)}{f_{\theta}(x)}\right)^2 $$

\pagebreak 
$$ E(\ddot{\ell}_x(\theta)) = - I_0 $$
\end{multicols}

MLE estimator of $\hat{\theta} : \hat{\theta}^{MLE}$
$$ \hat{\theta}^{MLE} \sim N\left(\theta,\frac{1}{I_0}\right) $$

\medskip
\emph{e.g.} for a normal dist. \\
let $x_i \sim N(\theta, \sigma^2)$ \\
* 1) compute ${\ell}_x(\theta)$ \\
density function $f_{\theta}(x) = \prod _{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x_i - \mu)^2}{2 \sigma^2}}$ \\
likelihood function ${\ell}_x(\theta) = -\frac{1}{2} \underset{i=1}{\overset{n}{\sum}} \frac{(x_i - \theta)^2}{\sigma^2} - \frac{n}{2} Log(2 \pi \sigma^2)$ \\
* 2) score function $\dot{\ell}_x(\theta) = \frac{1}{\sigma^2} \underset{i=1}{\overset{n}{\sum}} (x_i - \theta)$ \\
$\ddot{\ell}_x(\theta) = -\frac{n}{\sigma^2}$ \\
* 3) compute $I_0$ \\
as $E(\ddot{\ell}_x(\theta)) = - I_0$, Fisher Information $I_0 = \frac{n}{\sigma^2}$ \\
* 4) compute $\hat{\theta}^{MLE}$ \\
$E(\dot{\ell}_x(\theta)) = \frac{1}{\sigma^2} \underset{i=1}{\overset{n}{\sum}} (x_i - \theta) = 0$, such that \\
$\underset{i=1}{\overset{n}{\sum}} x_i = n \theta \Rightarrow \hat{\theta}^{MLE} = \frac{\underset{i=1}{\overset{n}{\sum}} x_i}{n} = \bar{x}$ \\
* 5) compute $se(\hat{\theta}^{MLE})$ \\
for a large n, \\
$\hat{\theta}^{MLE} \sim N\left(\theta,\frac{1}{I_0}\right) \Rightarrow \hat{\theta}^{MLE} \sim N\left(\theta,\frac{\sigma^2}{n}\right)$
$$ se(\hat{\theta}^{MLE}) = \frac{1}{I_0} = \frac{\sigma^2}{n} $$
* 6) $se(\hat{\theta}^{MLE}) = \frac{1}{n I_0}$, by Cramer-Rao lower bound. \\
The MLE has variance at least as small as the best unbiased estimate of $\theta$

% -----------------------------------------------------------------------
\subsubsection{Conditional Inference}
\emph{e.g.} An \emph{iid} sample $x \sim N(\theta, 0)$ has produced estimate $\hat{\theta} = \bar{x}$. however,
\begin{multicols}{2}
$$
n = \begin{cases} 25, & \text{prob } \frac{1}{2} \\ 100, & \text{prob } \frac{1}{2} \end{cases}
$$

\pagebreak 
$n = 25$ was declined
\end{multicols}

* Classical Frequentist rational: \\
$$ sd(\bar{x}) = \sigma_{\bar{x}} = \sqrt{\frac{1}{2} \frac{\sigma^2}{100} + \frac{1}{2} \frac{\sigma^2}{25}} = 0.158 $$

* Conditional Inference rational: \\
$$ sd(\bar{x}) = \sqrt{\frac{\sigma^2}{25}} = 0.2 $$

* use the likelihood function (based on observation) without the prior \\
* ``just take the sample you have" \\

1) more relevant inferences (w/what really happened) \\
2) simpler inferences (no correlation between the result and the sample size selection)

\medskip
\emph{e.g.} Observed Fisher Information $I_{(x)}$
$$ I_{(x)} = -\ddot{\ell_x}(\hat{\theta}^{MLE}) $$
In large samples $I_{(x)} = I_0$. Use $I_{(x)}$ in small samples 
$$ E[I_{(x)}] = n I_0 $$ 

* 1) compute the log-likelihood \\
$$ f_{\theta}(x) = \frac{1}{\pi} \frac{1}{1 + (x + \theta)^2} \Rightarrow \text{Cauchi density function} $$
$$ \ell_x(\theta) = Log\left(\frac{1}{\pi}\right) + Log(1) - Log(1 + (x + \theta)^2) $$
* 2) get its derivative \\
$$ \dot{\ell}_x(\theta) = \frac{2(x - \theta)}{1 + (x + \theta)^2} $$
* 3) get the 2nd derivative \\
$$ \ddot{\ell}_x(\theta) = \frac{-2(1 + (x - \theta)^2) + 4(x - \theta)^2}{(1 + (x - \theta)^2)^2} $$
* 4) get the observed fisher information \\
$$ I_{(x)} = -\ddot{\ell_x}(\hat{\theta}^{MLE}) $$
* 5) get the variance of the estimate, even if the distribution does not have a defined variance or expected value \\
- for 10000 samples of size $n$ with $\theta = 0$, compute $1/I_{(x)}$ and $\hat{\theta}^{MLE}$ \\
- group the 10000 $\hat{\theta}^{MLE}$ values according to quantiles of $1/I_{(x)}$ and calculate the empirical variance for each sample.

\medskip
* for all samples, the unconditional variance $1/n I_0$ is the same because all the samples are of the same size. \\
* on the other hand, $I_{(x)}$ will vary from sample to sample ($\hat{\theta}^{MLE}$ is different for each sample).
* $I_{(x)}$ is related to the variance.

% -----------------------------------------------------------------------
\subsubsection{Permutation and Randomization}

* when performing a t-test, it's assumed that the data samples come from a normal distribution.

* small samples may follow a different distribution. Randomization removes the normality assumption

* Randomization is: taking random groups from the data that are of the same size as the tested groups.

\medskip
* 1) compute the t-statistic for each randomly sampled pair of groups \\
* 2) get the t-statistic histogram

Utilizing random generated groups, it's expected the t-values not to be very high $\rightarrow$ construct an empirical distribution of t-values

% -----------------------------------------------------------------------
\subsection{Parametric Models and Exponential Families} % ---------------

% -----------------------------------------------------------------------
\subsubsection{Univariate Families}

\begin{center}
\begin{tabular}{ |ccccc| }
\hline
Name & Density & X & $\Omega$ & E \\
Notation &  &  &  & Var \\
\hline
Normal & $\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}}$ & $\mathbb{R}^{(1)}$ & $\mu \epsilon \mathbb{R}^{(1)}$ & $\mu$ \\
$N(\mu,\sigma^2)$ &  &  & $\sigma^2 \epsilon \mathbb{R}^{+}$ & $\sigma^2$ \\
\end{tabular}
\end{center}
* has two parameters, but they are very specific. $\mu$ is the location parameter, and $\sigma^2$ is the wide/narrow parameter \\
* model quatities that take positive and/or negative continuous values, if the distribution is symetric and if there are no too many extreme values

\begin{center}
\begin{tabular}{ |ccccc| }
\hline
Name & Density & X & $\Omega$ & E \\
Notation &  &  &  & Var \\
\hline
Poisson & $\frac{e^{-\lambda \lambda^x}}{x!}$ & $\mathbb{N}_{0}$ & $\lambda \epsilon \mathbb{R}^{+}$ & $\lambda$ \\
$Poi(\lambda)$ &  &  &  & $\lambda$ \\
\end{tabular}
\end{center}
* if the mean grows/shrinks the variance also grows/shrinks proportionally \\
* $\lambda$ must stay positive and is the interval of time of an exponential distribution, which is continuous $\rightarrow$ the expected number of successes can have decimals \\
* model a quantity that is discrete, it's the number of counts of something \\
* It's not very flexible as only has one parameter to tweak

\begin{center}
\begin{tabular}{ |ccccc| }
\hline
Name & Density & X & $\Omega$ & E \\
Notation &  &  &  & Var \\
\hline
Binomial & $\left(\text{}_x^n\right) \theta^n (1 - \theta)^{n - x}$ & $\{0,...$ & $0 \leq$ & $n \theta$ \\
$Bi(n, \theta)$ &  & $,n\}$ & $\theta \leq 1$ & $n \theta (1 - \theta)$ \\
\end{tabular}
\end{center}
* model the count of successes as Poisson, but we know the number of trials $n$

\begin{center}
\begin{tabular}{ |ccccc| }
\hline
Name & Density & X & $\Omega$ & E \\
Notation &  &  &  & Var \\
\hline
Gamma & $\frac{x^{\nu - 1} e^{-\frac{x}{\sigma}}}{\sigma^\nu \Gamma(\nu)}$ & $\mathbb{R}^{+}$ & $\nu > 0$ & $\sigma \nu$ \\
$Ga(\nu,\sigma)$ &  &  & $\sigma > 0$ & $\sigma^2 \nu$ \\
\end{tabular}
\end{center}
* the Gamma is used to model positive quantities. its common to use the inverse Gamma to model variances.

\begin{center}
\begin{tabular}{ |ccccc| }
\hline
Name & Density & X & $\Omega$ & E \\
Notation &  &  &  & Var \\
\hline
Beta & $\frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha,\beta)}$ & $0 \leq x \leq 1$ & $\alpha > 0$ & $\frac{\alpha}{\alpha + \beta}$ \\
$Be(\alpha,\beta)$ &  &  & $\beta > 0$ & $var$ \\
\end{tabular}
\end{center}
$var = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$ \\
* as x goes from 0 to 1, it's mostly used to talk about probabilities (aka. probability distribution)

\medskip
* both the Gamma and Beta have two parameters that convey some degree of flexibility \\
* Gamma is flexible but not as flexible as Beta \\
* The Binomial can approximate a Poisson with a large $n$ and small probability. \\

% -----------------------------------------------------------------------
\section{} % ------------------------------------------------------------
\hrule
Osamu Katagiri - A01212611, \href{https://www.linkedin.com/in/osamu-katagiri-84b2b940/}{linkedin.com/osamu-katagiri/}
\end{multicols}

\end{document}
