aovFitModel
#get test predictions using the LINEAR ANALYSIS
#We assume that: all values less than 0.5 correspond to the first category,
#0.5 or bigger correspond to the second category
lm_predictions = ifelse(predict(lmFitModel, mydata) > 0.5, "1", "0" )
#compare the actual data agains the predictions
correct_lm_predictions = mydata$test == lm_predictions
#count how many of the predictions are right vs wrong
#FALSE:WRONG TRUE:RIGHT
summary(correct_lm_predictions)
# * Confusion matrix
confusionMatrix(as.factor(lm_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
#get test predictions using the POLYNOMIAL ANALYSIS
#We assume that: all values less than 0.5 correspond to the first category,
#0.5 or bigger correspond to the second category
gp6_predictions = ifelse(predict(gp6Model, mydata) > 0.5, "1", "0" )
#compare the actual data agains the predictions
correct_gp6_predictions = mydata$test == gp6_predictions
#count how many of the predictions are right vs wrong
#FALSE:WRONG TRUE:RIGHT
summary(correct_gp6_predictions)
# * Confusion matrix
confusionMatrix(as.factor(gp6_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
#get test predictions using the ANOVA ANALYSIS
#We assume that: all values less than 0.5 correspond to the first category,
#0.5 or bigger correspond to the second category
aov_predictions = ifelse(predict(aovFitModel, mydata) > 0.5, "1", "0" )
#compare the actual data agains the predictions
correct_aov_predictions = mydata$test == aov_predictions
#count how many of the predictions are right vs wrong
#FALSE:WRONG TRUE:RIGHT
summary(correct_aov_predictions)
# * Confusion matrix
confusionMatrix(as.factor(aov_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
# Do a regression analysis to the "teengamb" dataset from the "faraway"
# library
data(teengamb)
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(teengamb)
# remove rows with incomplete data
mydata = teengamb[completeData, ]
str(mydata)
# Independent variables
X = model.matrix( ~ sex + status + income + verbal , data=teengamb )
# Response or dependent variable
y = teengamb$gamble
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
beta
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
# Residual Sum of Squares
RSS = t(error) %*% error
help("model.matrix")
# Do a regression analysis to the "teengamb" dataset from the "faraway"
# library
data(teengamb)
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(teengamb)
# remove rows with incomplete data
mydata = teengamb[completeData, ]
str(mydata)
names(mydata)
# Independent variables
X = model.matrix( ~ sex + status + income + verbal , data=teengamb )
# Response or dependent variable
y = teengamb$gamble
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
beta
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
yhat
error
# Residual Sum of Squares
RSS = t(error) %*% error
RSS
# Install package/library
# install.packages("faraway")
library(faraway)
data(pima)
# Exploration
head(pima)
summary(pima)
# Something weird out there?
#bmi, glucosa, triceps, insulin, diastolic != 0
# Test -> categoric data
plot(diabetes ~ as.factor(test), pima)
# Load library
library(faraway)
data(gala)
#Exploration
head(gala)
# Load library
library(faraway)
data(gala)
#Exploration
head(gala)
dim(gala)
# How do we set our matrix in R?
# Independent variables
X = model.matrix( ~ Endemics + Area + Elevation + Nearest + Scruz + Adjacent , data=gala )
# Response or dependent variable
y = gala$Species
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
beta
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
# Residual Sum of Squares
RSS = t(error) %*% error
# Compute the linear model using R functions
mdl = lm( Species ~ Endemics + Area + Elevation + Nearest + Scruz + Adjacent , data=gala)
summary(mdl)
## Lets ask for the rank of a matrix
library("Matrix")
rankMatrix(gala)
rankMatrix(gala)[1]
# Lets add a new variable
diff = gala$Nearest - gala$Scruz
gala$diff = diff
mdl = lm( Species ~ Endemics + Area + Elevation + Nearest + Scruz + Adjacent + diff, data=gala)
summary(mdl)
# Did the rank changed?
rankMatrix(gala)[1]
# Lets add a white noise
gala$diff = diff + rnorm( length(diff), sd = 0.0001 )
mdl = lm( Species ~ Endemics + Area + Elevation + Nearest + Scruz + Adjacent + diff, data=gala)
summary(mdl)
# Did the rank changed?
rankMatrix(gala)[1]
# Second part
## Polynomial regression
library(faraway)
data("corrosion")
head(corrosion)
gp2 = lm( loss ~ Fe + I(Fe^2), corrosion )
gp3 = lm( loss ~ Fe + I(Fe^2)+ I(Fe^3) , corrosion )
gp4 = lm( loss ~ Fe + I(Fe^2)+ I(Fe^3)+ I(Fe^4) , corrosion )
gp5 = lm( loss ~ Fe + I(Fe^2)+ I(Fe^3)+ I(Fe^4)+ I(Fe^5) , corrosion )
gp6 = lm( loss ~ Fe + I(Fe^2)+ I(Fe^3)+ I(Fe^4)+ I(Fe^5)+ I(Fe^6) , corrosion )
# predict(  lm estimated with R, data.frame with the new values to predict   (X matrix) )
grid = seq(0,2,len = 50)
plot(loss ~ Fe, data = corrosion, ylim=c(60, 140))
lines(grid, predict(gp2, data.frame(Fe=grid)) , col = "red")
plot(loss ~ Fe, data = corrosion, ylim=c(60, 140))
lines(grid, predict(gp3, data.frame(Fe=grid)) , col = "blue")
plot(loss ~ Fe, data = corrosion, ylim=c(60, 140))
lines(grid, predict(gp4, data.frame(Fe=grid)) , col = "green")
plot(loss ~ Fe, data = corrosion, ylim=c(60, 140))
lines(grid, predict(gp5, data.frame(Fe=grid)) , col = "magenta")
plot(loss ~ Fe, data = corrosion, ylim=c(60, 140))
lines(grid, predict(gp6, data.frame(Fe=grid)) , col = "chocolate")
# El mejor parece ser un polinomio de grado 6
plot(loss ~ Fe, data = corrosion, ylim=c(60, 130))
points(corrosion$Fe, fitted(gp6), pch = 18, col = "chocolate")
lines(grid, predict(gp6, data.frame(Fe=grid)) , col = "chocolate")
## ANCOVA
data(sexab)
head(sexab)
plot(ptsd~csa, sexab)
plot(ptsd~cpa, pch=as.character(csa), sexab)
## ANCOVA
data(sexab)
head(sexab)
g = lm(ptsd ~ cpa+csa, sexab)
summary(g)
# The base slope is 0.5506 (cpa)
# The second slope is -6.273 (csaNotAbused)
# Lets plot them!
plot(ptsd~cpa, pch=as.character(csa), sexab)
abline(10.2480, 0.5506, col = "red")
abline(10.2480-6.2728, 0.5506, col = "magenta")
## ANOVA
data("coagulation")
head(coagulation)
plot(coag ~ diet, coagulation, ylab="Coagulation time")
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#LINEAR ANALYSIS
# lmFitModel <- lm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age, data = mydata)
# summary(lmFitModel)
# lmFitModel
#Let's remove triceps, insulin, and age as they do not have a significant impact on the model
lmFitModel <- lm(test ~ pregnant + glucose + diastolic + bmi + diabetes, data = mydata)
summary(lmFitModel)
lmFitModel
#POLYNOMIAL ANALYSIS
gp6Model = lm(
test ~
(pregnant + glucose + diastolic + bmi + diabetes)    +
I((pregnant + glucose + diastolic + bmi + diabetes)^2) +
I((pregnant + glucose + diastolic + bmi + diabetes)^3) +
I((pregnant + glucose + diastolic + bmi + diabetes)^4) +
I((pregnant + glucose + diastolic + bmi + diabetes)^5) +
I((pregnant + glucose + diastolic + bmi + diabetes)^6),
data = mydata)
summary(gp6Model)
gp6Model
#get test predictions using the POLYNOMIAL ANALYSIS
#We assume that: all values less than 0.5 correspond to the first category,
#0.5 or bigger correspond to the second category
gp6_predictions = ifelse(predict(gp6Model, mydata) > 0.5, "1", "0" )
#compare the actual data agains the predictions
correct_gp6_predictions = mydata$test == gp6_predictions
#count how many of the predictions are right vs wrong
#FALSE:WRONG TRUE:RIGHT
summary(correct_gp6_predictions)
# * Confusion matrix
confusionMatrix(as.factor(gp6_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
#get test predictions using the ANOVA ANALYSIS
#We assume that: all values less than 0.5 correspond to the first category,
#0.5 or bigger correspond to the second category
aov_predictions = ifelse(predict(aovFitModel, mydata) > 0.5, "1", "0" )
#compare the actual data agains the predictions
correct_aov_predictions = mydata$test == aov_predictions
#count how many of the predictions are right vs wrong
#FALSE:WRONG TRUE:RIGHT
summary(correct_aov_predictions)
# * Confusion matrix
confusionMatrix(as.factor(aov_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
<<<<<<< Updated upstream
#<<<<<<< Updated upstream
# Do a regression analysis to the âpimaâ dataset from the âfarawayâ library
=======
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
#>>>>>>> Stashed changes
library(faraway)
library(faraway)
library(caret)
data(pima)
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#LOGISTIC REGRESSION
glmFitModel <- glm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age, data = mydata)
summary(glmFitModel)
#Let's remove triceps, insulin, and age as they do not have a significant impact on the model
glmFitModel <- glm(test ~ pregnant + glucose + diastolic + bmi + diabetes, data = mydata)
summary(glmFitModel)
glmFitModel
#get test predictions using the LOGISTIC REGRESSION
glm_predictions = predict(glmFitModel, mydata)
#We assume that: all values less than 0.5 correspond to the first category, 0.5 or bigger correspond to the second category
glm_predictions = ifelse(glm_predictions > 0.5, 1, 0)
# get the percentage of prediction ...
variableTable = table(glm_predictions , as.character(mydata$test))
sum(diag(variableTable))/sum(variableTable)
#count how many of the predictions are right vs wrong
correct_lm_predictions = mydata$test == glm_predictions
#FALSE:WRONG TRUE:RIGHT
summary(correct_lm_predictions)
# * Confusion matrix to see the model performance
confusionMatrix(as.factor(glm_predictions), as.factor(mydata$test), positive = NULL, dnn = c("Prediction", "Reference"))
# Which regression did you use?
# * ANCOVA, ANOVA, simple regression, logistic regression
# * Justify your answer
sprintf("Logistic regression, since logistic regression is better used when the dependent variable is categorical.")
# Do a regression analysis to the "teengamb" dataset from the "faraway"
# library
library(faraway)
data(teengamb)
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(teengamb)
# remove rows with incomplete data
mydata = teengamb[completeData, ]
str(mydata)
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#There is data from 2 sex, labeled 0, 1
#Let's convert the sex data into a factor
mydata$sex = factor(as.numeric(mydata$sex))
# Independent variables
X = model.matrix( ~ sex + status + income + verbal + income:sex, mydata)
# Response or dependent variable
y = mydata$gamble
# Independent variables
X = model.matrix( ~ sex + status + income + verbal , data=teengamb )
# Response or dependent variable
y = teengamb$gamble
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
# Residual Sum of Squares
RSS = t(error) %*% error
sprintf("RSS = %f", as.double(RSS))
# R-squared
numerator = sum((yhat-y)^2)
denominator = sum((y - mean(yhat))^2)
R.squared = 1 - (numerator/denominator)
sprintf("R^2 = %f", as.double(R.squared))
beta
# Estimated output
yhat = X %*% beta
yhat
# Estimation Error
error = y - yhat
error
# Residual Sum of Squares
RSS = t(error) %*% error
RSS
# * Use the R function to compare your answers
# Compute the linear model using R functions
lmFitModel = lm(formula = gamble ~ sex + status + income + verbal + income:sex, data = mydata)
#print the model's summary to verify the R-squared calculation
summary(lmFitModel)
# Analysis of Variance Model to verify the RSS calculation
aov(lmFitModel)
#Let's remove sex, status, verbal and sex as they do not have a significant impact on the model
lmFitModel = lm(formula = gamble ~ income + income:sex, data = mydata)
sprintf("ANCOVA, since the predictors are a mixture of quantitative and qualitative.")
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
# Residual Sum of Squares
RSS = t(error) %*% error
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
<<<<<<< Updated upstream
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
# Residual Sum of Squares
RSS = t(error) %*% error
sprintf("RSS = %f", as.double(RSS))
# Which regression did you use?
# * ANCOVA, ANOVA, simple regression, logistic regression
# * Justify your answer
sprintf("ANCOVA, since the predictors are a mixture of quantitative and qualitative.")
# Which regression did you use?
# * ANCOVA, ANOVA, simple regression, logistic regression
# * Justify your answer
"ANCOVA, since the predictors are a mixture of quantitative and qualitative."
"ANCOVA, since the predictors are a mixture of quantitative and qualitative."
help(f)
help(sprintf)
R.squared = 1 - (numerator/denominator)
R.squared
# Do a regression analysis to the "teengamb" dataset from the "faraway"
# library
library(faraway)
data(teengamb)
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(teengamb)
# remove rows with incomplete data
mydata = teengamb[completeData, ]
str(mydata)
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#There is data from 2 sex, labeled 0, 1
#Let's convert the sex data into a factor
mydata$sex = factor(as.numeric(mydata$sex))
# Independent variables
X = model.matrix( ~ sex + status + income + verbal + income:sex, mydata)
# Response or dependent variable
y = mydata$gamble
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
# Estimated output
yhat = X %*% beta
# Estimation Error
error = y - yhat
# Residual Sum of Squares
RSS = t(error) %*% error
sprintf("RSS = %f", as.double(RSS))
# R-squared
numerator = sum((yhat-y)^2)
denominator = sum((y - mean(yhat))^2)
R.squared = 1 - (numerator/denominator)
R.squared
sprintf("R^2 = %f", as.double(R.squared))
# * Use the R function to compare your answers
# Compute the linear model using R functions
lmFitModel = lm(formula = gamble ~ sex + status + income + verbal + income:sex, data = mydata)
#print the model's summary to verify the R-squared calculation
summary(lmFitModel)
# Analysis of Variance Model to verify the RSS calculation
aov(lmFitModel)
#Let's remove sex, status, verbal and sex as they do not have a significant impact on the model
lmFitModel = lm(formula = gamble ~ income + income:sex, data = mydata)
# Which regression did you use?
# * ANCOVA, ANOVA, simple regression, logistic regression
# * Justify your answer
sprintf("ANCOVA, since the predictors are a mixture of quantitative and qualitative.")
help(sprintf)
# Do a regression analysis to the "teengamb" dataset from the "faraway"
# library
library(faraway)
data(teengamb)
# complete.cases() returns a logical vector with the value TRUE for rows
# that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(teengamb)
# remove rows with incomplete data
mydata = teengamb[completeData, ]
str(mydata)
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
# Lets apply the least squares formula
# ( (X' X)^-1 * x  X'  * y
# ?solve -> is is used to solve the inverse of a matrix
beta = solve(t(X) %*% X) %*% t(X) %*% y
beta
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#LOGISTIC REGRESSION
glmFitModel <- glm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age, data = mydata)
summary(glmFitModel)
glmFitModel
#get test predictions using the LOGISTIC REGRESSION
glm_predictions = predict(glmFitModel, mydata)
#We assume that: all values less than 0.5 correspond to the first category, 0.5 or bigger correspond to the second category
glm_predictions = ifelse(glm_predictions > 0.5, 1, 0)
# get the percentage of prediction ...
variableTable = table(glm_predictions , as.character(mydata$test))
sum(diag(variableTable))/sum(variableTable)
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
head(mydata)
head(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
