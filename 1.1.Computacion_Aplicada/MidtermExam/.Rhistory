# Apply filter by filter
index = pima$glucose > 0
pima = pima[ index,  ]
index = pima$diastolic > 0
pima = pima[ index,  ]
index = pima$insulin > 0
pima = pima[ index,  ]
index = pima$triceps > 0
pima = pima[ index,  ]
index = pima$bmi > 0
pima = pima[ index,  ]
filteredPima = pima
# New size
dim(filteredPima) # 392 x 9
summary(filteredPima)
# Set numeric values to categoric value
filteredPima$test <- as.factor(filteredPima$test)
summary(filteredPima)
# Which linear model am I going to use?
# Logistic regression
mdl <- glm( test ~ ., data = filteredPima, family = "binomial")
summary(mdl)
mdl <- glm( test ~ glucose + bmi + diabetes + age, data = filteredPima, family = "binomial")
summary(mdl)
# Lets see the performance of our beta coefficients
predicted <- predict(mdl, filteredPima[,-9], type = "response")
hist(predicted)
predicted <- ifelse(predicted < 0.5, "0", "1")
predicted.table = table("Real" = filteredPima[,9], "Predicted" = predicted)
print(predicted.table)
# Prediction Power
print(sum(diag(predicted.table))/sum(predicted.table))
## Part 2
data("teengamb")
head(teengamb)
summary(teengamb)
# ANCOVA regression
teengamb$sex <- as.factor(teengamb$sex)
summary(teengamb)
# Lets create our inependent variable matrix and dependent variable vector
X <- model.matrix( ~  sex + status + income + verbal, data=teengamb )
y <- teengamb$gamble
# First formula
beta <- solve( t(X) %*% X) %*% t(X) %*% y
# Estimates
yhat <- X %*% beta
error <- y - yhat
# Residual Sum of Squares
RSS <- t(error) %*% error
# R-squared
numerator = sum( ( yhat - y)^2 )
denominator = sum( ( y - mean(y))^2 )
R.squared = 1 - numerator/denominator
# Using R function
mdl <- lm( gamble ~ . , data=teengamb)
summary(mdl)
# Lets compare
print(beta)
# Using R function
mdl <- lm( gamble ~ . , data=teengamb)
summary(mdl)
# Lets compare
print(beta)
print(RSS)
print(R.squared)
?mdl
?glm
# First we define a variable to call for the database to be analized:
chocolate = read.csv("Chocolate.csv")
head(chocolate)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(chocolate)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(choco_data)
summary(choco_data)
## Part 1
library(faraway)
data(pima)
# Exploration
summary(pima)
# Apply filter by filter
index = pima$glucose > 0
pima = pima[ index,  ]
index = pima$diastolic > 0
pima = pima[ index,  ]
index = pima$insulin > 0
pima = pima[ index,  ]
index = pima$triceps > 0
pima = pima[ index,  ]
index = pima$bmi > 0
pima = pima[ index,  ]
filteredPima = pima
# New size
dim(filteredPima) # 392 x 9
summary(filteredPima)
# Set numeric values to categoric value
filteredPima$test <- as.factor(filteredPima$test)
summary(filteredPima)
# Which linear model am I going to use?
# Logistic regression
mdl <- glm( test ~ ., data = filteredPima, family = "binomial")
summary(mdl)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(choco_data) # Verifying there is no repeated information.
summary(choco_data)
mdl <- glm(formula = chocolate ~ ., family = "binomial", data = choco_data)
summary(mdl)
View(choco_data)
View(choco_data)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(choco_data) # Verifying there is no repeated information.
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Filtering our data in order to concentrate in our dependent variable: "Chocolate":
index <- choco_data$chocolate > 0
filteredData <- choco_data[index,]
filteredData
mdl <- glm(formula = chocolate ~ ., family = "binomial", data = choco_data)
mdl <- glm(formula = chocolate ~ ., family = "binomial", data = filteredData)
summary(mdl)
hist(filteredData)
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
# remove rows containing zeros
filter <- with(mydata,
glucose   > 0 &
diastolic > 0 &
triceps   > 0 &
insulin   > 0 &
bmi       > 0)
mydata <- mydata[filter,]
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
#LOGISTIC REGRESSION
glmFitModel <- glm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age, data = mydata)
summary(glmFitModel)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
head(choco_data) # Verifying there is no repeated information.
str(choco_data) # Exploring the data.
mdl <- glm(formula = chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, family = "binomial", data = filteredData)
mdl <- glm(formula = chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = filteredData)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Filtering our data in order to concentrate in our dependent variable: "Chocolate":
index <- choco_data$chocolate > 0
filteredData <- choco_data[index,]
mdl <- glm(formula = chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = filteredData)
summary(mdl)
## Part 2
data("teengamb")
head(teengamb)
summary(teengamb)
# ANCOVA regression
teengamb$sex <- as.factor(teengamb$sex)
summary(teengamb)
# Lets create our inependent variable matrix and dependent variable vector
X <- model.matrix( ~  sex + status + income + verbal, data=teengamb )
y <- teengamb$gamble
# First formula
beta <- solve( t(X) %*% X) %*% t(X) %*% y
# Estimates
yhat <- X %*% beta
error <- y - yhat
# Residual Sum of Squares
RSS <- t(error) %*% error
# R-squared
numerator = sum( ( yhat - y)^2 )
denominator = sum( ( y - mean(y))^2 )
R.squared = 1 - numerator/denominator
# Using R function
mdl <- lm( gamble ~ . , data=teengamb)
summary(mdl)
# Lets compare
print(beta)
print(RSS)
print(R.squared)
# There is no need to filter our data since all variables are categorical, with the exceptions of "sugarpercent", "pricepercent" and "winpercent" but none of them have any row with a value of 0.
?boxplot
boxplot(~chocolate, data = choco_data)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# There is no need to filter our data since all variables are categorical, with the exceptions of "sugarpercent", "pricepercent" and "winpercent" but none of them have any row with a value of 0.
?boxplot
boxplot(~chocolate, data = choco_data)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
mdl <- glm(formula = chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = filteredData)
mdl <- glm(formula = chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data)
summary(mdl) # Visualizing the model to determine the most significant variables.
# Do a regression analysis to the "pima" dataset from the "faraway"
# library
library(faraway)
library(caret)
data(pima)
# * Analyze the database and select only the observations with no missing
#   data
# complete.cases() returns a logical vector with the value TRUE for rows that are complete, and FALSE for rows that have some NA values
completeData = complete.cases(pima)
# remove rows with incomplete data
mydata = pima[completeData, ]
# remove rows containing zeros
filter <- with(mydata,
glucose   > 0 &
diastolic > 0 &
triceps   > 0 &
insulin   > 0 &
bmi       > 0)
mydata <- mydata[filter,]
#Let's take a look to the data ...
str(mydata)
pairs(mydata)
cor(mydata)
summary(mydata)
hist(choco_data)
?hist
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
mdl <- glm.fit(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
## Part 1
library(faraway)
data(pima)
# Exploration
summary(pima)
# Apply filter by filter
index = pima$glucose > 0
pima = pima[ index,  ]
index = pima$diastolic > 0
pima = pima[ index,  ]
index = pima$insulin > 0
pima = pima[ index,  ]
index = pima$triceps > 0
pima = pima[ index,  ]
index = pima$bmi > 0
pima = pima[ index,  ]
filteredPima = pima
# New size
dim(filteredPima) # 392 x 9
summary(filteredPima)
# Set numeric values to categoric value
filteredPima$test <- as.factor(filteredPima$test)
summary(filteredPima)
# Which linear model am I going to use?
# Logistic regression
mdl <- glm( test ~ ., data = filteredPima, family = "binomial")
summary(mdl)
mdl <- glm( test ~ glucose + bmi + diabetes + age, data = filteredPima, family = "binomial")
summary(mdl)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Since the dependent variable "Chocolate" is a categorical variable, we have decided to use Logistic Regression:
mdl <- glm(chocolate ~ ., data = choco_data, family = "binomial")
summary(mdl)
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
# Lets see the performance of our beta coefficients:
predicted <- predict(mdl, choco_data[,-9], type = "response")
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Since the dependent variable "Chocolate" is a categorical variable, we have decided to use Logistic Regression:
mdl <- glm(chocolate ~ ., data = choco_data, family = "binomial")
summary(mdl)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Set numeric values to categoric value
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
# Set numeric values to categoric value
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
# Since the dependent variable "Chocolate" is a categorical variable, we have decided to use Logistic Regression:
mdl <- glm(chocolate ~ ., data = choco_data, family = "binomial")
summary(mdl)
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
## Part 1
library(faraway)
data(pima)
# Exploration
summary(pima)
# Apply filter by filter
index = pima$glucose > 0
pima = pima[ index,  ]
index = pima$diastolic > 0
pima = pima[ index,  ]
index = pima$insulin > 0
pima = pima[ index,  ]
# Setting numeric values to categoric value:
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
?factor
?levels()
?labels()
?label()
mdl <- glm(chocolate ~ competitorname +fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
chocolate_labels <- choco_data[,1]
# Setting numeric values to categoric value:
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
chocolate_labels <- choco_data[,1]
choco_data <- choco_data[,-1]
# Setting numeric values to categoric value:
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
# Since the dependent variable "Chocolate" is a categorical variable, we have decided to use Logistic Regression:
mdl <- glm(chocolate ~ ., data = choco_data, family = "binomial")
summary(mdl)
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
# Lets see the performance of our beta coefficients:
predicted <- predict(mdl, choco_data[,], type = "response")
hist(predicted)
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
chocolate_labels <- choco_data[,1]
choco_data <- choco_data[,-1]
# Setting numeric values to categoric value:
choco_data$chocolate <- as.factor(choco_data$chocolate)
summary(choco_data)
# Since the dependent variable "Chocolate" is a categorical variable, we have decided to use Logistic Regression:
mdl <- glm(chocolate ~ ., data = choco_data, family = "binomial")
summary(mdl)
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
# Lets see the performance of our beta coefficients:
predicted <- predict(mdl, choco_data[,], type = "response")
hist(predicted)
mdl <- glm(chocolate ~ fruity + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
mdl <- glm(chocolate ~ fruity + caramel +  peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
mdl <- glm(chocolate ~ fruity + winpercent, data = choco_data, family = "binomial")
summary(mdl) # Visualizing the model to determine the most significant variables.
# Lets see the performance of our beta coefficients:
predicted <- predict(mdl, choco_data[,], type = "response")
hist(predicted)
#We assume that: all values less than 0.5 correspond to the first category, 0.5 or bigger correspond to the second category
glm_predicted = ifelse(predicted > 0.5, 1, 0)
hist(glm_predicted)
taylorPlot <- function(f, c, from, to) {
# Plot the Taylor approximations up to the 2nd, 4th, 6th and 8th terms
#
# Parameters
# ----------
# f : function
# Vectorized function of one variable
# c : numeric
# point where the series expansion will take place
# from, to : numeric
# Interval of points to be ploted
#
# Returns
# -------
# void
x <- seq(from, to, length.out = 100)
yf <- f(x)
yp2 <- polyval(taylor(f, c, 2), x)
yp4 <- polyval(taylor(f, c, 4), x)
yp6 <- polyval(taylor(f, c, 6), x)
yp8 <- polyval(taylor(f, c, 8), x)
plot(
x,
yf,
xlab = "x",
ylab = "f(x)",
type = "l",
main = ' Taylor Series Approximation of f(x) ',
col = "black",
lwd = 2
)
lines(x, yp2, col = "#c8e6c9")
lines(x, yp4, col = "#81c784")
lines(x, yp6, col = "#4caf50")
lines(x, yp8, col = "#388e3c")
legend(
'topleft',
inset = .05,
legend = c("TS 8 terms", "TS 6 terms", "TS 4 terms", "TS 2 terms", "f(x)"),
col = c('#388e3c', '#4caf50', '#81c784', '#c8e6c9', 'black'),
lwd = c(1),
bty = 'n',
cex = .75
)
}
f0 <- function(x) {
res = sin(x)
return(res)
}
f1 <- function(x) {
res = exp(complex(real = 0, imaginary = 1)*x)
return(res)
}
taylorPlot(f0, 0, -6.6, 6.6)
taylorPlot(f1, 1, -2*pi, 2*pi)
library(pracma)
taylorPlot <- function(f, c, from, to) {
# Plot the Taylor approximations up to the 2nd, 4th, 6th and 8th terms
#
# Parameters
# ----------
# f : function
# Vectorized function of one variable
# c : numeric
# point where the series expansion will take place
# from, to : numeric
# Interval of points to be ploted
#
# Returns
# -------
# void
x <- seq(from, to, length.out = 100)
yf <- f(x)
yp2 <- polyval(taylor(f, c, 2), x)
yp4 <- polyval(taylor(f, c, 4), x)
yp6 <- polyval(taylor(f, c, 6), x)
yp8 <- polyval(taylor(f, c, 8), x)
plot(
x,
yf,
xlab = "x",
ylab = "f(x)",
type = "l",
main = ' Taylor Series Approximation of f(x) ',
col = "black",
lwd = 2
)
lines(x, yp2, col = "#c8e6c9")
lines(x, yp4, col = "#81c784")
lines(x, yp6, col = "#4caf50")
lines(x, yp8, col = "#388e3c")
legend(
'topleft',
inset = .05,
legend = c("TS 8 terms", "TS 6 terms", "TS 4 terms", "TS 2 terms", "f(x)"),
col = c('#388e3c', '#4caf50', '#81c784', '#c8e6c9', 'black'),
lwd = c(1),
bty = 'n',
cex = .75
)
}
f0 <- function(x) {
res = sin(x)
return(res)
}
f1 <- function(x) {
res = exp(complex(real = 0, imaginary = 1)*x)
return(res)
}
taylorPlot(f0, 0, -6.6, 6.6)
taylorPlot(f1, 1, -2*pi, 2*pi)
?pracma
# First we define a variable to call for the database to be analized:
choco_data = read.csv("Chocolate.csv")
str(choco_data) # Verifying there is no repeated information.
summary(choco_data) # Exploring the data.
chocolate_labels <- choco_data[,1]
choco_data <- choco_data[,-1]
# Setting numeric values to categoric value:
choco_data$chocolate <- as.factor(choco_data$chocolate)
